{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "martial-relationship",
   "metadata": {},
   "source": [
    "   **<center><h1>ML Assignment 1</h1></center>**\n",
    "\n",
    "<center><h3>Srijan Mallick</h3></center>\n",
    "\n",
    "<center><h3>February 28, 2021</h3></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**<h3>Theorem</h3>**\n",
    "\n",
    "*Under Gaussian assumption linear regression amounts to least square (ordinary least square)*\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**<u>Proof</u>:**\n",
    "\n",
    "In a linear regression problem, our primary objective is to estimate the regression coefficients by minimising the error term. In this case, we consider our loss function as the square of the error terms, and the cost function J is the loss, averaged over the entire dataset. In this section, we will give a set of probabilistic assumptions, under\n",
    "which least-squares regression is derived.\n",
    "\n",
    "The required linear model is given by: \n",
    "\n",
    "$$ y_i = \\theta^Tx_i +\\varepsilon_i \\;,\\; \\text{where} \\;\\; \\varepsilon_i \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\sigma^2)$$\n",
    "</br>\n",
    "\n",
    "Here $\\varepsilon_i$ is the error term for each individual data point, which captures the error in prediction, or some random noise.\n",
    "The density of $\\varepsilon_i$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "    &p(\\varepsilon_i) = \\frac{1}{\\sigma\\sqrt{2\\pi}} exp\\left(-\\frac{\\varepsilon_i^2}{2\\sigma^2}\\right)    \n",
    "    \\\\\\Rightarrow \\;\\;\\;\n",
    "    &p(y_i - \\theta^Tx_i) = \\frac{1}{\\sigma\\sqrt{2\\pi}} exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2}\\right]    \n",
    "\\end{align*}\n",
    "\n",
    "The conventional way to right this probability is:\n",
    "\n",
    "$$p(y_i\\;|\\;x_i;\\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2}\\right]$$ \n",
    "\n",
    "where the notation in RHS denotes the distribution of $y_i$ given $x_i$, parametrised by $\\theta$. When viewed as a function of $\\theta$, we call it the **likelihood function**, given by \n",
    "\n",
    "\\begin{align*}\n",
    "    L(\\theta) \n",
    " &= L(\\theta\\;|\\;x_i,\\vec{y})\\\\ \n",
    " &= p(\\vec{y}\\;|\\;X;\\theta) \\\\\n",
    " &= \\prod_{i=1}^{m}p(y_i|x_i;\\theta)\\\\\n",
    " &= \\prod_{i=1}^{m}\\frac{1}{\\sigma\\sqrt{2\\pi}} exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Instead of maximizing L($\\theta$) , we can also maximize any strictly increasing\n",
    " function of L($\\theta$). So instead we maximize the **log likelihood function**.\n",
    " \n",
    "Let the data be given by $\\mathcal{D} = (x_i , y_i)_{i=1}^{n}$ \n",
    "With Bayes theorem we compute $\\theta$ from data $\\mathcal{D}$ \n",
    "\n",
    "\\begin{align*}\n",
    "    p(\\theta|\\mathcal{D}) \n",
    "    &=\\frac{p(\\mathcal{D}|\\theta).p(\\theta)}{p(\\mathcal{D})} \n",
    "    &=\\frac{\\textbf{L}(\\theta|\\mathcal{D}).p(\\theta)}{p(\\mathcal{D})}\n",
    "\\end{align*}\n",
    "\n",
    "$p(\\mathcal{D}|\\theta)$ is a function of $\\theta$ given $\\mathcal{D}$ as we want to choose that particular $\\theta$ which will maximize the probability i.e. the **Maximum Likelihood Estimator**.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta^* &= \\underset{\\theta}{argmax}\\;\\;\\; L(\\theta|\\mathcal{D})\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\;\\;  p(\\mathcal{D}| \\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\;\\; p(y_1,x_1,y_2,x_2,y_3,x_3,\\dots,y_m,x_m\\;;\\;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\prod_{i=1}^{m} p(y_i,x_i\\;;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\; \\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta) p(x_i\\;;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\; \\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta) p(x_i)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\; \\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\; \\sum_{i=1}^{m} log\\;[p(y_i|x_i\\;;\\theta)]\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\; \\sum_{i=1}^{m}\\log\\frac{1}{\\sigma\\sqrt{2\\pi}} exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2}\\right]\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\;\\ \\frac{-1}{\\sigma^2}\\;\\;\\textbf{.}\\;\\;\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\\\\n",
    "    &= \\underset{\\theta}{argmin} \\;\\;\\frac{1}{m}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "which we recognize to be **J**($\\theta$), our original least-squares cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-scenario",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-cartoon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
